{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eafbedaf-3cc9-4c6b-a552-4f5d70bd47bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8d9704-567a-4bbd-ba8f-c7c69a3101f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ba0cac-4dfc-4ce9-9a0b-572808055298",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce328ca-8d78-452c-a2f2-0c80f36ff571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7662916-4603-4316-986f-9df00ccccedc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_n = [4, 8, 16, 32]\n",
    "features_for_3 = []\n",
    "features_for_2 = []\n",
    "for n in features_n:\n",
    "  for k in features_n:\n",
    "    for s in features_n:\n",
    "      features_for_3.append([n,k,s])\n",
    "      \n",
    "for n in features_n:\n",
    "  for k in features_n:\n",
    "    features_for_2.append([n,k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2585569c-945a-42fd-9e2f-068f70966c1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(features_for_3)\n",
    "print(features_for_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f457bac9-ddd9-4fea-ae17-41529cba2435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test sets\n",
    "layers = [2,3]\n",
    "batchSizes = [8, 16, 32, 64] #[8,32,64]\n",
    "kernelSizes = [3,5,7,9]\n",
    "\n",
    "features = [ [4, 4, 4], [4, 4, 8], [4, 4, 16], [4, 4, 32], [4, 8, 4], [4, 8, 8], [4, 8, 16], [4, 8, 32], [4, 16, 4], [4, 16, 8], [4, 16, 16], [4, 16, 32], [4, 32, 4], [4, 32, 8], [4, 32, 16], [4, 32, 32], [8, 4, 4], [8, 4, 8], [8, 4, 16], [8, 4, 32], [8, 8, 4], [8, 8, 8], [8, 8, 16], [8, 8, 32], [8, 16, 4], [8, 16, 8], [8, 16, 16], [8, 16, 32], [8, 32, 4], [8, 32, 8], [8, 32, 16], [8, 32, 32], [16, 4, 4], [16, 4, 8], [16, 4, 16], [16, 4, 32], [16, 8, 4], [16, 8, 8], [16, 8, 16], [16, 8, 32], [16, 16, 4], [16, 16, 8], [16, 16, 16], [16, 16, 32], [16, 32, 4], [16, 32, 8], [16, 32, 16], [16, 32, 32], [32, 4, 4], [32, 4, 8], [32, 4, 16], [32, 4, 32], [32, 8, 4], [32, 8, 8], [32, 8, 16], [32, 8, 32], [32, 16, 4], [32, 16, 8], [32, 16, 16], [32, 16, 32], [32, 32, 4], [32, 32, 8], [32, 32, 16], [32, 32, 32]]\n",
    "# \n",
    "\n",
    "features_for_2_display = [[4, 4], [4, 8], [4, 16], [4, 32], [8, 4], [8, 8], [8, 16], [8, 32], [16, 4], [16, 8], [16, 16], [16, 32], [32, 4], [32, 8], [32, 16], [32, 32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dabec5d-259f-46e7-a454-42d956f659df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "955e4e95-e5ad-4561-a18b-6ef8ec365e0c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ebd233-4451-465f-8858-e7634da7de42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "# Example usage:\n",
    "zip_path = '/Volumes/scratch/walt_wu/ganesh_project/test_data_before_cropped/Bin23.zip'\n",
    "extract_to = '/Volumes/scratch/walt_wu/ganesh_project/test_data_before_cropped/'\n",
    "\n",
    "# Ensure the extract directory exists\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "unzip_file(zip_path, extract_to)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce29413-ee06-4d22-a0d4-47a34eba6b17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory):\n",
    "    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
    "\n",
    "# Example usage:\n",
    "directory = '/Volumes/scratch/walt_wu/ganesh_project/test_data_after_cropped/'\n",
    "file_count = count_files_in_directory(directory)\n",
    "print(f'There are {file_count} files in the directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9ddaf4-0a6e-4880-b8d6-b56f63445229",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Remove dodgy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d2670c-4778-46d0-9442-3b57b0593a41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import imghdr\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589ec0d1-0782-407a-9688-a3947578df92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/scratch/walt_wu/ganesh_project/2407704/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b586f2c5-ddde-4536-ac78-2cf2927d7e62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_exts = ['jpeg','jpg', 'bmp', 'png']\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3abb10f9-aae0-49c8-a047-37d566054e59",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start = 0\n",
    "# for slots in os.listdir(data_dir): \n",
    "#     # print(slots)\n",
    "#     for type_of_errors in os.listdir(os.path.join(data_dir, slots)):\n",
    "#         print(type_of_errors)\n",
    "#         for image in os.listdir(os.path.join(data_dir, slots, type_of_errors)):\n",
    "#             if type_of_errors == 'A':\n",
    "#                 start += 1\n",
    "#                 image_path = os.path.join(data_dir, slots, type_of_errors, image) \n",
    "#                 shutil.copy(image_path, f'/Volumes/scratch/walt_wu/ganesh_project/Combine_dataset/A/{str(start)}_{image}')\n",
    "#                 # print(image_path)\n",
    "#                 # try: \n",
    "#                 #     img = cv2.imread(image_path)\n",
    "#                 #     # if not img:\n",
    "#                 #     #     print(image_path)\n",
    "#                 #         # os.remove(image_path)\n",
    "#                 #     tip = imghdr.what(image_path)\n",
    "#                 #     if tip not in image_exts: \n",
    "#                 #         print('Image not in ext list {}'.format(image_path))\n",
    "#                         # os.remove(image_path)\n",
    "#             elif type_of_errors == 'B':\n",
    "#                 start += 1\n",
    "#                 image_path = os.path.join(data_dir, slots, type_of_errors, image) \n",
    "#                 shutil.copy(image_path, f'/Volumes/scratch/walt_wu/ganesh_project/Combine_dataset/B/{str(start)}_{image}')\n",
    "#             elif type_of_errors == 'C':\n",
    "#                 start += 1\n",
    "#                 image_path = os.path.join(data_dir, slots, type_of_errors, image)  \n",
    "#                 shutil.copy(image_path, f'/Volumes/scratch/walt_wu/ganesh_project/Combine_dataset/C/{str(start)}_{image}')\n",
    "#             elif type_of_errors == 'O':\n",
    "#                 start += 1\n",
    "#                 image_path = os.path.join(data_dir, slots, type_of_errors, image) \n",
    "#                 shutil.copy(image_path, f'/Volumes/scratch/walt_wu/ganesh_project/Combine_dataset/O/{str(start)}_{image}')\n",
    "#                 # except Exception as e: \n",
    "#                 #     print('Issue with image {}'.format(image_path))\n",
    "#                 #     os.remove(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f0a7c0-551c-4340-93c0-a0364ca48692",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation for center cropping\n",
    "crop_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda img: transforms.functional.center_crop(img, (300, 300))),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "# Define the source and destination directories\n",
    "folder_path = '/Volumes/scratch/walt_wu/ganesh_project/test_data_before_cropped/Bin23/'\n",
    "destination = '/Volumes/scratch/walt_wu/ganesh_project/test_data_after_cropped/'\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "# Iterate over each folder in the source directory\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    # Extract the folder name\n",
    "    print(root, dirs, files)\n",
    "    folder_name = os.path.basename(root)\n",
    "    \n",
    "    # Create the corresponding destination folder if it doesn't exist\n",
    "    dest_folder = os.path.join(destination, folder_name)\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each file in the folder\n",
    "    for filename in files:\n",
    "        # Construct the full path to the image file\n",
    "        filepath = os.path.join(root, filename)\n",
    "        \n",
    "        # Open the image\n",
    "        img_pil = Image.open(filepath)\n",
    "        \n",
    "        # Apply the cropping transformation\n",
    "        cropped_img_pil = crop_transform(img_pil)\n",
    "        \n",
    "        # Construct the destination path for the cropped image\n",
    "        dest_filepath = os.path.join(dest_folder, filename)\n",
    "        \n",
    "        # Save the cropped image\n",
    "        cropped_img_pil.save(dest_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f19198-e2c2-401b-97ab-0bd7ec12e298",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "def is_valid_image(file_path):\n",
    "    try:\n",
    "        img = Image.open(file_path)\n",
    "        img.verify()\n",
    "        return True\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        return False\n",
    "      \n",
    "\n",
    "for root, dirs, files in os.walk('/Volumes/scratch/walt_wu/ganesh_project/test_data_after_cropped/'):\n",
    "  for filename in files:\n",
    "        # Construct the full path to the image file\n",
    "    filepath = os.path.join(root, filename)\n",
    "    # try: \n",
    "    if is_valid_image(filepath):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Invalid image.\" + filepath)\n",
    "        os.remove(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae62f04-366e-4b0a-b367-1faab907ac70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for root, dirs, files in os.walk('/Volumes/scratch/walt_wu/ganesh_project/Cropped_Combine_dataset_400/O/'):\n",
    "  for filename in files:\n",
    "        # Construct the full path to the image file\n",
    "        filepath = os.path.join(root, filename)\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "        plt.imshow(mpimg.imread(filepath))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c240dfc-8bbe-4b4d-b148-974233918d50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(f'/Volumes/scratch/walt_wu/ganesh_project/Cropped_Combine_dataset/O/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da00a73-ec07-44b0-aea7-755c10a7ac27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(len(os.listdir('/Volumes/scratch/walt_wu/ganesh_project/1 - Copy/A/')))\n",
    "# start = len(os.listdir('/Volumes/scratch/walt_wu/ganesh_project/2407704/1 - Copy/O/'))\n",
    "# for i in range (2, 25):\n",
    "#   if i == 12:\n",
    "#     continue\n",
    "#   print(len(os.listdir(f'/Volumes/scratch/walt_wu/ganesh_project/2407704/1 - Copy ({i})/O')))\n",
    "#   start += len(os.listdir(f'/Volumes/scratch/walt_wu/ganesh_project/2407704/1 - Copy ({i})/O'))\n",
    "# print(start)\n",
    "print(len(os.listdir('/Volumes/scratch/walt_wu/ganesh_project/Combine_dataset/O/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f222fb45-a444-4073-ad5f-c1e1b70b99f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "new_folder = '/Volumes/scratch/walt_wu/ganesh_project/Cropped_Combine_dataset'\n",
    "folders = ['A','B','C','O']\n",
    "\n",
    "if not os.path.exists(new_folder):\n",
    "    os.makedirs(new_folder)\n",
    "for folder in folders:\n",
    "  os.makedirs(new_folder+f'/{folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43dc53a5-e642-45ef-82e4-72e81331e5a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # %fs rm -r '/Volumes/scratch/walt_wu/ganesh_project/2407704/1 - Copy (12)/D/'\n",
    "# def delete_all_contents_in_folder(folder_path):\n",
    "#     for root, dirs, files in os.walk(folder_path, topdown=False):\n",
    "#         for name in files:\n",
    "#             os.remove(os.path.join(root, name))\n",
    "#         for name in dirs:\n",
    "#             os.rmdir(os.path.join(root, name))\n",
    "# delete_all_contents_in_folder('/Volumes/scratch/walt_wu/ganesh_project/Combine_dataset/A/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a706481a-d7ec-4aa4-98d7-ece5cd576118",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5e4e65-6b60-41ba-9dae-434500f053b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ba9139-139e-4c82-81c4-91065e2c0e70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tf.data.Dataset??\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88806cf6-379a-4d08-a933-11e8bc5359dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image as a PIL image\n",
    "img_pil = Image.open('/Volumes/scratch/walt_wu/ganesh_project/2407704/1 - Copy/A/image2.png')\n",
    "\n",
    "# Convert the PIL image to a PyTorch tensor\n",
    "img_tensor = transforms.ToTensor()(img_pil)\n",
    "\n",
    "# Define the desired output size for the crop\n",
    "output_size = (200, 200)  # Example: 100x100 pixels\n",
    "\n",
    "\n",
    "# Apply the center crop transformation\n",
    "cropped_img_tensor = transforms.functional.center_crop(img_tensor, output_size)\n",
    "\n",
    "# Convert the cropped tensor back to a PIL image\n",
    "cropped_img_pil = transforms.ToPILImage()(cropped_img_tensor)\n",
    "\n",
    "# Display the original image\n",
    "plt.imshow(img_pil)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display the cropped image\n",
    "plt.imshow(cropped_img_pil)\n",
    "plt.title(\"Cropped Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245fd9b8-e6ca-47b1-af52-e47945209183",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='/Volumes/scratch/walt_wu/ganesh_project/test_data_after_cropped/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=30,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    "    pad_to_aspect_ratio=False,\n",
    "    data_format=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Extract class names (optional)\n",
    "class_names = data.class_names\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot_encode_labels(images, labels):\n",
    "    return images, tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# Apply one-hot encoding to the dataset (not sure why)\n",
    "one_hot_dataset = data.map(one_hot_encode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380990bd-6866-4613-b539-24a73b3f47c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "one_hot_dataset\n",
    "print(one_hot_dataset)\n",
    "# print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef77e6e-b045-47f3-8c3b-a41a4d512d9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a26afce-b5ba-4d61-9e3c-b0ce3f49496b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_iterator = data.as_numpy_iterator()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e197494-229e-4144-ae94-26d27f98dc95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch = data_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b07d58-3e09-4ff3-9d6e-e39e61787d44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239f6b07-50e0-4caa-9b27-419d404c2101",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "print(ax)\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    print(idx)\n",
    "    print(img)\n",
    "    # Normalize the image data if it's not already in the correct range\n",
    "    if img.max() > 1:\n",
    "        img = img / img.max()\n",
    "    elif img.dtype == np.uint8:\n",
    "        img = img.astype(np.float64) / 255\n",
    "    \n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].set_title(batch[1][idx])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd53c83b-be3f-4821-80dc-a845a3f86195",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fce7894-a015-4526-86d9-a2a53bda0bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaled_data = one_hot_dataset.map(lambda x,y: (x/255, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44a4c30-469a-4ca9-93b2-4f20963aaf6c",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaled_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6a473a3-726a-4928-9209-97f07305a41e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767c0bcf-f70d-40f5-8a61-c68314097da2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d289a7a-d3de-427a-ac5d-b12383e16bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_size = int((len(scaled_data)-1)*.7)\n",
    "val_size = int((len(scaled_data))*.2)\n",
    "test_size = len(scaled_data) - int((len(scaled_data)-1)*.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c627200f-b3b5-4c8f-8a70-29cbb40a17cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9c2466-b879-4175-9049-b81e39dae0b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = scaled_data.take(train_size)\n",
    "val = scaled_data.skip(train_size).take(val_size)\n",
    "test = scaled_data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29017228-a687-4573-9e41-23210bf99e24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(type(data))\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8c92a22-b48f-4512-bd3c-4a71488b006f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 6. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6fb1df-be2f-4981-80c2-8454fd7485be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2035aac4-a077-4d1e-a02b-d8eef3ec7f96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beae61fa-cb00-4ae5-980e-2c76f2c30ace",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54407a9-8d11-449c-85bf-b3dfb1e4462b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# randomly choose 3 layers as my start\n",
    "# Add Convolutional and Max Pooling layers\n",
    "model.add(Conv2D(16, (3, 3), strides=1, activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32, (3, 3), strides=1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(16, (3, 3), strides=1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "# Flatten the tensor output from the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a Dense layer with 256 units and ReLU activation\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# Add the output layer with 4 units (for 4 classes) and softmax activation\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Set a threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8d3862-493d-4a6d-a5ba-878ab2ca88e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c1a829-8471-4491-9702-745c11178eb1",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffa94e10-54a1-4fae-b772-b7b60199f8cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "#Read cars data from csv\n",
    "data = pd.read_csv(\"cars.csv\")\n",
    "\n",
    "#Set marker properties\n",
    "markersize = data['engine-size']/12\n",
    "markercolor = data['city-mpg']\n",
    "markershape = data['num-of-doors'].replace(\"four\",\"square\").replace(\"two\",\"circle\")\n",
    "\n",
    "\n",
    "#Make Plotly figure\n",
    "fig1 = go.Scatter3d(x=data['curb-weight'],\n",
    "                    y=data['horsepower'],\n",
    "                    z=data['price'],\n",
    "                    marker=dict(size=markersize,\n",
    "                                color=markercolor,\n",
    "                                symbol=markershape,\n",
    "                                opacity=0.9,\n",
    "                                reversescale=True,\n",
    "                                colorscale='Blues'),\n",
    "                    line=dict (width=0.02),\n",
    "                    mode='markers')\n",
    "\n",
    "#Make Plot.ly Layout\n",
    "mylayout = go.Layout(scene=dict(xaxis=dict( title=\"curb-weight\"),\n",
    "                                yaxis=dict( title=\"horsepower\"),\n",
    "                                zaxis=dict(title=\"price\")),)\n",
    "\n",
    "#Plot and save html\n",
    "plotly.offline.plot({\"data\": [fig1],\n",
    "                     \"layout\": mylayout},\n",
    "                     auto_open=True,\n",
    "                     filename=(\"6DPlot.html\"))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e002882-5171-4775-9813-c23598c3fa66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783fc0b3-3738-45a9-ac2a-6827d7ff1483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logdir='logs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be4ef2d-fe1e-4cd2-b91a-31284f0468b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "880ac0a1-948b-4803-a01b-4acc5e870d31",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# print(train)\n",
    "# print(val)\n",
    "hist = model.fit(train, validation_data=val, epochs=20, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56368592-c07e-4b33-a2d3-018cb3666dbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd65956c-30f1-43d8-9759-1b841d4fb8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 8. Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97a5a8c-4928-436f-a739-27cbf10da24e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d5b81d-30f4-4dd7-a424-f9613d2e52bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming hist.history['accuracy'] and hist.history['val_accuracy'] are defined elsewhere\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "\n",
    "# Set the title of the figure\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Adjust the x-ticks to increment by 1\n",
    "plt.xticks(range(len(hist.history['accuracy'])))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fb07575-8550-42ba-b2cd-e01b3b70102c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 9. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d1b954-df17-430f-8b32-550aba7f1668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5919bb-226a-46e4-a15d-52676c14f72f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84630d46-fbcd-474f-9252-88bf0fe95cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator(): \n",
    "    X, y = batch\n",
    "    yhat = model.predict(X)\n",
    "    pre.update_state(y, yhat)\n",
    "    re.update_state(y, yhat)\n",
    "    acc.update_state(y, yhat)\n",
    "\n",
    "print(pre.result(), re.result(), acc.result())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca62afc9-c5f8-466a-8410-96e2e2612656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming test_as_numpy_iterator is an iterator over batches of your test data\n",
    "# and model is your trained model\n",
    "\n",
    "num_classes = 4\n",
    "correct_predictions = 0\n",
    "\n",
    "for batch in test.as_numpy_iterator():\n",
    "    X, y = batch\n",
    "    yhat = model.predict(X)\n",
    "    correct_predictions = 0\n",
    "    counter = 0\n",
    "    # Interpret model outputs: Choose the class with the highest probability\n",
    "    predicted_classes = np.argmax(yhat, axis=-1)\n",
    "    predicted_indices = np.argmax(yhat, axis=-1)\n",
    "\n",
    "    # Construct one-hot encoded arrays\n",
    "    one_hot_encoded_pred = np.zeros((len(yhat), num_classes))  # Initialize a zeros array\n",
    "    one_hot_encoded_pred[np.arange(len(yhat)), predicted_indices] = 1 \n",
    "\n",
    "    # Compare predicted classes to true labels\n",
    "    for pred, true_label in zip(one_hot_encoded_pred, y):\n",
    "        counter += 1\n",
    "        # if [0. 0. 0. 1.] == \n",
    "        if np.array_equal(pred, true_label):\n",
    "            np.array(np.array)\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(20,20))\n",
    "            ax.imshow(X[counter])\n",
    "            # one_hot_dataset.map(lambda x,y: (x/255, y))\n",
    "            ax.title.set_text(y[counter])\n",
    "            print( f\"Model prediction:{pred}, My label: {true_label}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "    accuracy = correct_predictions / len(y)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cff5fc4-0093-4353-b289-4b6dbf296b8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "input = torch.tensor([0, 2, 1, 3])\n",
    "target = torch.tensor([0, 1, 2, 3])\n",
    "multiclass_confusion_matrix(input, target, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3807a02-a0d4-486e-90a1-b56c49cd9dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "print(ax)\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    print(idx)\n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f29d85-50f2-4e1a-8593-ea90929bb3f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming yhat is your model's output\n",
    "yhat = [\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [9.9999619e-01, 3.7844698e-06, 5.6748788e-16, 5.8578958e-10],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [9.9877292e-01, 4.3484211e-04, 1.5276953e-07, 7.9211750e-04],\n",
    "    [0.0, 1.0, 0.0, 0.0]\n",
    "]\n",
    "\n",
    "# Find the index of the maximum value in each prediction array\n",
    "predicted_indices = np.argmax(yhat, axis=-1)\n",
    "\n",
    "# Construct one-hot encoded arrays\n",
    "one_hot_encoded = np.zeros((len(yhat), num_classes))  # Initialize a zeros array\n",
    "one_hot_encoded[np.arange(len(yhat)), predicted_indices] = 1  # Set the index to 1\n",
    "\n",
    "# Print the one-hot encoded predictions\n",
    "print(one_hot_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "801bc7b4-180b-449e-b11d-a3b27a7d51dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 10. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0024c1b-047f-4c93-a0ed-82fc682aaa29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install opencv-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00ba955-d4f9-468c-9434-b5c2fb07be33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('154006829.jpg')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8af155-4b1e-4628-a91f-4ccbbfb33d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resize = tf.image.resize(img, (256,256))\n",
    "plt.imshow(resize.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66763f38-9115-451b-9997-2792ac0fa0b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yhat = model.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b640f6-2b2f-4136-af0a-04add4395174",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53b63257-8253-4261-8a8e-2aa8a7b0ee32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if yhat > 0.5: \n",
    "    print(f'Predicted class is Sad')\n",
    "else:\n",
    "    print(f'Predicted class is Happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e774f180-b7bc-433c-a576-8c5a7d14a996",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f840dcc3-e396-4eaa-bdc3-370879d55860",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2ca1b2-7647-4f26-b645-78889205c88d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model.save(os.path.join('models','imageclassifier_3_layers_300*300.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b893119-ae4b-4a98-8aa5-0a2dd60c0525",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_model = load_model('/Workspace/Users/walt.wu@qorvo.com/Project with Ganesh/models/imageclassifier_3_layers.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b324ddc-8763-442d-83c4-0571bfef34ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_model.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Loop experimenting and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test logics: loop around dataset batch_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Place this at the beginning of your notebook or cell to ensure inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "vis_table = pd.DataFrame()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for batchSize in batchSizes:\n",
    "    for kernelSize in kernelSizes:\n",
    "        for feature in features:\n",
    "            print(f'The following result is for batchSize {batchSize}, kernelSize {kernelSize}, and feature {feature}. Having 300*300 as cropped and has 3 layers')\n",
    "            data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                directory='/Volumes/scratch/walt_wu/ganesh_project/Cropped_Combine_dataset_300/',\n",
    "                labels='inferred',\n",
    "                label_mode='int',\n",
    "                class_names=None,\n",
    "                color_mode='rgb',\n",
    "                batch_size=batchSize,\n",
    "                image_size=(300, 300),\n",
    "                shuffle=True,\n",
    "                seed=None,\n",
    "                validation_split=None,\n",
    "                subset=None,\n",
    "                interpolation='bilinear',\n",
    "                follow_links=False,\n",
    "                crop_to_aspect_ratio=False,\n",
    "                pad_to_aspect_ratio=False,\n",
    "                data_format=None,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            # Extract class names (optional)\n",
    "            class_names = data.class_names\n",
    "\n",
    "            # Determine the number of classes\n",
    "            num_classes = len(class_names)\n",
    "\n",
    "            # One-hot encode the labels\n",
    "            def one_hot_encode_labels(images, labels):\n",
    "                return images, tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "            # Apply one-hot encoding to the dataset (not sure why)\n",
    "            one_hot_dataset = data.map(one_hot_encode_labels)\n",
    "            scaled_data = one_hot_dataset.map(lambda x,y: (x/255, y))\n",
    "            train_size = int((len(scaled_data)-1)*.7)\n",
    "            val_size = int((len(scaled_data))*.2)\n",
    "            test_size = len(scaled_data) - int((len(scaled_data)-1)*.1)\n",
    "            print(train_size, val_size, test_size)\n",
    "            train_data = scaled_data.take(train_size) \n",
    "            val_data = scaled_data.skip(train_size).take(val_size)  \n",
    "            test_data = scaled_data.skip(train_size).skip(val_size) \n",
    "\n",
    "            # Initialize the model\n",
    "            model = Sequential()\n",
    "\n",
    "            # Add Convolutional and Max Pooling layers\n",
    "            model.add(Conv2D(feature[0], (kernelSize, kernelSize), strides=1, activation='relu', input_shape=(300, 300, 3)))\n",
    "            model.add(MaxPooling2D())\n",
    "            model.add(Conv2D(feature[1], (kernelSize, kernelSize), strides=1, activation='relu'))\n",
    "            model.add(MaxPooling2D())\n",
    "            model.add(Conv2D(feature[2], (kernelSize, kernelSize), strides=1, activation='relu'))\n",
    "            model.add(MaxPooling2D())\n",
    "\n",
    "            # Flatten the tensor output from the convolutional layers\n",
    "            model.add(Flatten())\n",
    "\n",
    "            # Add a Dense layer with 256 units and ReLU activation\n",
    "            model.add(Dense(256, activation='relu'))\n",
    "\n",
    "            # Add the output layer with 4 units (for 4 classes) and softmax activation\n",
    "            model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            logdir='logs' \n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "            # Fit the model\n",
    "            hist = model.fit(train_data, validation_data=val_data, epochs=20, callbacks=[tensorboard_callback])\n",
    "\n",
    "            # Plot Loss\n",
    "            # Ensure inline plotting\n",
    "            %matplotlib inline\n",
    "\n",
    "            # Plot Loss\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the folder path where you want to save the plots\n",
    "\n",
    "\n",
    "            folder_path = '/Volumes/scratch/walt_wu/ganesh_project/300*300_3layers_new_batch8/'\n",
    "            vis_table[f'{batchSize}_{kernelSize}_{feature}_loss'] = hist.history['loss']\n",
    "            vis_table[f'{batchSize}_{kernelSize}_{feature}_val_loss'] = hist.history['val_loss']\n",
    "            vis_table[f'{batchSize}_{kernelSize}_{feature}_accuracy'] = hist.history['accuracy']\n",
    "            vis_table[f'{batchSize}_{kernelSize}_{feature}_val_accuracy'] = hist.history['val_accuracy']\n",
    "            import pandas as pd\n",
    "\n",
    "            # Assume df is your dataframe\n",
    "            # Save the dataframe to a CSV file with header\n",
    "\n",
    "            import os\n",
    "\n",
    "            file_path = '/Volumes/scratch/walt_wu/ganesh_project/300*300_3layers_new_batch8/vis_table.csv'\n",
    "\n",
    "            # Check if the file exists\n",
    "            if os.path.exists(file_path):\n",
    "                # Remove the file if it exists\n",
    "                os.remove(file_path)\n",
    "\n",
    "            # Save the dataframe to a CSV file with header\n",
    "            vis_table.to_csv(file_path, index=False)\n",
    "\n",
    "            # Plot Loss and save the figure\n",
    "            fig = plt.figure()\n",
    "            plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "            plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "            fig.suptitle('Loss', fontsize=20)\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            plt.savefig(f'{folder_path}{batchSize}_{kernelSize}_{feature}_loss_plot.png')\n",
    "            plt.close(fig)  # Close the figure to free up resources\n",
    "\n",
    "            # Plot Accuracy and save the figure\n",
    "            fig = plt.figure()\n",
    "            plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "            plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "            fig.suptitle('Accuracy', fontsize=20)\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            plt.savefig(f'{folder_path}{batchSize}_{kernelSize}_{feature}_accuracy_plot.png')\n",
    "            plt.close(fig)  # Close the figure to free up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden Path Testing (best feature combination)\n",
    "\n",
    "batchSizes = [8,16] #[8,32,64]  \n",
    "kernelSizes = [3]  \n",
    "feature = [[32, 32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "The following result is for batchSize 16, kernelSize 3, and feature [32, 32]. Having 300*300 as cropped and has 2 layers iteration\n",
      "Found 751 files belonging to 4 classes.\n",
      "32 9 43\n",
      "<_SkipDataset element_spec=(TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:720)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:439)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:439)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1297)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:1023)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:979)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:787)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:813)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:812)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:867)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:660)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:549)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:527)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:107)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:107)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:89)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "batchSizes = [16]  # Reduced batch sizes\n",
    "kernelSizes = [3]\n",
    "features = [[32, 32]]\n",
    "\n",
    "train_table = pd.DataFrame()\n",
    "test_CM = pd.DataFrame()\n",
    "test_result = pd.DataFrame()\n",
    "\n",
    "# Configure TensorFlow to allocate memory dynamically\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "for batchSize in batchSizes:\n",
    "    for kernelSize in kernelSizes:\n",
    "        for feature in features:\n",
    "            print(f'The following result is for batchSize {batchSize}, kernelSize {kernelSize}, and feature {feature}. Having 300*300 as cropped and has 2 layers iteration')\n",
    "            data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                directory='/Volumes/scratch/walt_wu/ganesh_project/Cropped_Combine_dataset_300/',\n",
    "                labels='inferred',\n",
    "                label_mode='int',\n",
    "                class_names=None,\n",
    "                color_mode='rgb',\n",
    "                batch_size=batchSize,\n",
    "                image_size=(300, 300),\n",
    "                shuffle=True,\n",
    "                seed=None,\n",
    "                validation_split=None,\n",
    "                subset=None,\n",
    "                interpolation='bilinear',\n",
    "                follow_links=False,\n",
    "                crop_to_aspect_ratio=False,\n",
    "                pad_to_aspect_ratio=False,\n",
    "                data_format=None,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            # Extract class names (optional)\n",
    "            class_names = data.class_names\n",
    "\n",
    "            # Determine the number of classes\n",
    "            num_classes = len(class_names)\n",
    "\n",
    "            # One-hot encode the labels\n",
    "            def one_hot_encode_labels(images, labels):\n",
    "                return images, tf.one_hot(labels, depth=num_classes)\n",
    "\n",
    "            # Apply one-hot encoding to the dataset\n",
    "            one_hot_dataset = data.map(one_hot_encode_labels)\n",
    "            scaled_data = one_hot_dataset.map(lambda x, y: (x/255, y))\n",
    "            train_size = int((len(scaled_data)-1)*.7)\n",
    "            val_size = int((len(scaled_data))*.2)\n",
    "            test_size = len(scaled_data) - int((len(scaled_data)-1)*.1)\n",
    "            print(train_size, val_size, test_size)\n",
    "            train_data = scaled_data.take(train_size)\n",
    "            val_data = scaled_data.skip(train_size).take(val_size)\n",
    "            test_data = scaled_data.skip(train_size).skip(val_size)\n",
    "            print(test_data)\n",
    "\n",
    "            # Initialize the model\n",
    "            model = Sequential()\n",
    "\n",
    "            # Add Convolutional and Max Pooling layers\n",
    "            model.add(Conv2D(feature[0], (kernelSize, kernelSize), strides=1, activation='relu', input_shape=(300, 300, 3)))\n",
    "            model.add(MaxPooling2D())\n",
    "            model.add(Conv2D(feature[1], (kernelSize, kernelSize), strides=1, activation='relu'))\n",
    "            model.add(MaxPooling2D())\n",
    "\n",
    "            # Flatten the tensor output from the convolutional layers\n",
    "            model.add(Flatten())\n",
    "\n",
    "            # Add a Dense layer with 256 units and ReLU activation\n",
    "            model.add(Dense(256, activation='relu'))\n",
    "\n",
    "            # Add the output layer with `num_classes` units and softmax activation\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            logdir = 'logs'\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "            # Fit the model\n",
    "            hist = model.fit(train_data, validation_data=val_data, epochs=20, callbacks=[tensorboard_callback])\n",
    "\n",
    "            # Define the folder path where you want to save the plots\n",
    "            folder_path = '/Volumes/scratch/walt_wu/ganesh_project/test_round/'\n",
    "            train_table[f'{batchSize}_{kernelSize}_{feature}_loss'] = hist.history['loss']\n",
    "            train_table[f'{batchSize}_{kernelSize}_{feature}_val_loss'] = hist.history['val_loss']\n",
    "            train_table[f'{batchSize}_{kernelSize}_{feature}_accuracy'] = hist.history['accuracy']\n",
    "            train_table[f'{batchSize}_{kernelSize}_{feature}_val_accuracy'] = hist.history['val_accuracy']\n",
    "\n",
    "            file_path = f'/Volumes/scratch/walt_wu/ganesh_project/test_round/{batchSize}_{kernelSize}_{feature}_train_table.csv'\n",
    "\n",
    "            # Check if the file exists\n",
    "            if os.path.exists(file_path):\n",
    "                # Remove the file if it exists\n",
    "                os.remove(file_path)\n",
    "\n",
    "            # Save the model\n",
    "            model_save_path = os.path.join('models', 'imageclassifier_2_layers_300*300.h5')\n",
    "            model.save(model_save_path)\n",
    "            print(f'Model saved to {model_save_path}')\n",
    "            break\n",
    "            print(f'Model saved to {model_save_path}')\n",
    "\n",
    "            # Save the dataframe to a CSV file with header\n",
    "            train_table.to_csv(file_path, index=False)\n",
    "            print(f'Train table saved to {file_path}')\n",
    "\n",
    "            # Plot Loss and save the figure\n",
    "            fig = plt.figure()\n",
    "            plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "            plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "            fig.suptitle('Loss', fontsize=20)\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            loss_plot_path = f'{folder_path}{batchSize}_{kernelSize}_{feature}_loss_plot.png'\n",
    "            plt.savefig(loss_plot_path)\n",
    "            plt.close(fig)\n",
    "            print(f'Loss plot saved to {loss_plot_path}')\n",
    "\n",
    "            # Plot Accuracy and save the figure\n",
    "            fig = plt.figure()\n",
    "            plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "            plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "            fig.suptitle('Accuracy', fontsize=20)\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            accuracy_plot_path = f'{folder_path}{batchSize}_{kernelSize}_{feature}_accuracy_plot.png'\n",
    "            plt.savefig(accuracy_plot_path)\n",
    "            plt.close(fig)\n",
    "            print(f'Accuracy plot saved to {accuracy_plot_path}')\n",
    "\n",
    "            test_result_l = []\n",
    "            correct = []\n",
    "            total_sample = []\n",
    "\n",
    "            # Create directory to save misclassified images if it doesn't exist\n",
    "            misclassified_dir = folder_path\n",
    "            if not os.path.exists(misclassified_dir):\n",
    "                os.makedirs(misclassified_dir)\n",
    "\n",
    "            print(f'Test starting')\n",
    "\n",
    "            # Initialize variables to track total correct predictions and total samples\n",
    "            total_correct_predictions = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            y_true_all = []\n",
    "            y_pred_all = []\n",
    "\n",
    "            # Iterate over batches in test_data\n",
    "            for batch in test_data.as_numpy_iterator():\n",
    "                X, y = batch\n",
    "                yhat = model.predict(X)\n",
    "                # Interpret model outputs: Choose the class with the highest probability\n",
    "                predicted_indices = np.argmax(yhat, axis=-1)\n",
    "\n",
    "                y_true_labels = np.argmax(y, axis=-1)\n",
    "\n",
    "                y_true_all.extend(y_true_labels)\n",
    "                y_pred_all.extend(predicted_indices)\n",
    "\n",
    "                # Compare predicted classes to true labels\n",
    "                for g, (pred, true_label) in enumerate(zip(predicted_indices, y_true_labels)):\n",
    "                    total_samples += 1\n",
    "                    if pred == true_label:\n",
    "                        total_correct_predictions += 1\n",
    "                    else:\n",
    "                        # Plot the misclassified image\n",
    "                        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                        ax.imshow(X[g])\n",
    "                        ax.title.set_text(f'True: {true_label}, Predicted: {pred}')\n",
    "\n",
    "                        # Save the plot with a meaningful filename\n",
    "                        filename = os.path.join(misclassified_dir, f\"test_sample_{g}_true_{true_label}_pred_{pred}.png\")\n",
    "                        plt.savefig(filename)\n",
    "                        plt.close(fig)  # Close the figure to free up memory\n",
    "\n",
    "                        print(f\"Saved misclassified image: {filename}\")\n",
    "\n",
    "                # Calculate overall accuracy\n",
    "                overall_accuracy = total_correct_predictions / total_samples\n",
    "                print(f\"Overall Model Accuracy for test : {overall_accuracy:.4f}\")\n",
    "                test_result_l.append(overall_accuracy)\n",
    "                correct.append(total_correct_predictions)\n",
    "                total_sample.append(total_samples)\n",
    "                test_result[f'overall_accuracy_for_{batchSize}_{kernelSize}_{feature}_iteration_test'] = [overall_accuracy]\n",
    "                test_result[f'total_correct_predictions_for_{batchSize}_{kernelSize}_{feature}_iteration_test'] = [total_correct_predictions]\n",
    "                test_result[f'total_samples_for_{batchSize}_{kernelSize}_{feature}_iteration_test'] = [total_samples]\n",
    "\n",
    "                # Compute confusion matrix\n",
    "                conf_mat = confusion_matrix(y_true_all, y_pred_all, labels=list(range(num_classes)))\n",
    "                conf_mat_df = pd.DataFrame(conf_mat, index=class_names, columns=class_names)\n",
    "                test_CM[f'conf_mat_{batchSize}_{kernelSize}_{feature}_iter_test_'] = conf_mat_df.values.flatten()\n",
    "\n",
    "            save_path = f'{folder_path}{batchSize}_{kernelSize}_{feature}_iteration_test_results.csv'\n",
    "            if os.path.exists(save_path):\n",
    "                os.remove(save_path)\n",
    "            test_result.to_csv(save_path, index=False)\n",
    "            print(f'Test results saved to {save_path}')\n",
    "\n",
    "            # Save confusion matrices\n",
    "            conf_mat_save_path = f'{folder_path}confusion_matrix_{batchSize}_{kernelSize}_{feature}_iteration.csv'\n",
    "            if os.path.exists(conf_mat_save_path):\n",
    "                os.remove(conf_mat_save_path)\n",
    "            test_CM.to_csv(conf_mat_save_path, index=False)\n",
    "            print(f'Confusion matrix saved to {conf_mat_save_path}')\n",
    "\n",
    "# Additional code to make predictions on a folder of images and save the count of each category to a CSV file\n",
    "def predict_and_save_counts(model, image_folder_path, output_csv_path, image_size=(300, 300)):\n",
    "    # Get a list of all image file paths in the folder\n",
    "    image_files = [os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    # Function to preprocess images\n",
    "    def preprocess_image(img_path):\n",
    "        img = image.load_img(img_path, target_size=image_size)\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        img_array = img_array / 255.0  # Normalize to [0, 1] range\n",
    "        return img_array\n",
    "\n",
    "    # Initialize a Counter to keep track of predictions\n",
    "    prediction_counter = Counter()\n",
    "\n",
    "    # Iterate over all images and make predictions\n",
    "    for img_path in image_files:\n",
    "        img_array = preprocess_image(img_path)\n",
    "        prediction = model.predict(img_array)\n",
    "        predicted_class = np.argmax(prediction, axis=-1)[0]  # Get the class with the highest probability\n",
    "        prediction_counter[predicted_class] += 1\n",
    "\n",
    "    # Convert the counter to a pandas DataFrame\n",
    "    prediction_df = pd.DataFrame.from_dict(prediction_counter, orient='index', columns=['Count'])\n",
    "    prediction_df.index.name = 'Class'\n",
    "    prediction_df.reset_index(inplace=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    prediction_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f'Prediction counts saved to {output_csv_path}')\n",
    "\n",
    "# Example usage of the predict_and_save_counts function\n",
    "image_folder_path = '/Volumes/scratch/walt_wu/ganesh_project/test_data_after_cropped/'\n",
    "output_csv_path = '/Volumes/scratch/walt_wu/ganesh_project/test_round/predictions_count.csv'\n",
    "predict_and_save_counts(model, image_folder_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type B Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_B/16876740264.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876747246.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type B Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_B/16876747383.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876747523.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876747731.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876747812.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876747815.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876747940.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876748233.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876748466.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876748495.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876748585.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876748662.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876748877.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876748905.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876748916.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876749353.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876749403.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876749425.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876749439.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876750003.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876750096.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876750329.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876751121.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876751162.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876751515.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876751606.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876756966.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16876756996.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type B Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_B/16876757009.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type B Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_B/16876757019.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0\n",
      "\n",
      "*** WARNING: max output size exceeded, skipping output. ***\n",
      "\n",
      "\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16877006356.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16877006423.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16877006439.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16877006459.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16877025543.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type B Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_B/16877025599.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16877025623.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Saved Type D Error image: /Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/16877025639.png\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Prediction counts saved to /Volumes/scratch/walt_wu/ganesh_project/test_round/predictions_count.csv\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Load the trained model\n",
    "model_path = '/Workspace/Users/walt.wu@qorvo.com/Project with Ganesh/models/imageclassifier_2_layers_300*300.h5'\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Define the folder path where your images are stored\n",
    "image_folder_path = '/Volumes/scratch/walt_wu/ganesh_project/test_data_after_cropped/'\n",
    "\n",
    "# Define the image size (should match the input size of the model)\n",
    "image_size = (300, 300)\n",
    "\n",
    "# Get a list of all image file paths in the folder\n",
    "image_files = [os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=image_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize to [0, 1] range\n",
    "    return img_array\n",
    "\n",
    "# Directories to save images predicted as type D and type B errors\n",
    "type_d_error_dir = '/Volumes/scratch/walt_wu/ganesh_project/test_round/type_D/'\n",
    "type_b_error_dir = '/Volumes/scratch/walt_wu/ganesh_project/test_round/type_B/'\n",
    "if not os.path.exists(type_d_error_dir):\n",
    "    os.makedirs(type_d_error_dir)\n",
    "if not os.path.exists(type_b_error_dir):\n",
    "    os.makedirs(type_b_error_dir)\n",
    "\n",
    "# Initialize a Counter to keep track of predictions\n",
    "prediction_counter = Counter()\n",
    "\n",
    "# Iterate over all images and make predictions\n",
    "for img_path in image_files:\n",
    "    img_array = preprocess_image(img_path)\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction, axis=-1)[0]  # Get the class with the highest probability\n",
    "\n",
    "    # Increment the counter for the predicted class\n",
    "    prediction_counter[predicted_class] += 1\n",
    "\n",
    "    # Check if the prediction corresponds to type D error (class 3)\n",
    "    if predicted_class == 3:  # Assuming class 3 corresponds to type D\n",
    "        img = image.load_img(img_path, target_size=image_size)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Predicted as Type D Error')\n",
    "        \n",
    "        # Save the image\n",
    "        filename = os.path.basename(img_path)\n",
    "        save_path = os.path.join(type_d_error_dir, filename)\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f'Saved Type D Error image: {save_path}')\n",
    "    \n",
    "    # Check if the prediction corresponds to type B error (class 1)\n",
    "    if predicted_class == 1:  # Assuming class 1 corresponds to type B\n",
    "        img = image.load_img(img_path, target_size=image_size)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Predicted as Type B Error')\n",
    "        \n",
    "        # Save the image\n",
    "        filename = os.path.basename(img_path)\n",
    "        save_path = os.path.join(type_b_error_dir, filename)\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f'Saved Type B Error image: {save_path}')\n",
    "\n",
    "# Convert the counter to a pandas DataFrame\n",
    "prediction_df = pd.DataFrame.from_dict(prediction_counter, orient='index', columns=['Count'])\n",
    "prediction_df.index.name = 'Class'\n",
    "prediction_df.reset_index(inplace=True)\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_path = '/Volumes/scratch/walt_wu/ganesh_project/test_round/predictions_count.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "prediction_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f'Prediction counts saved to {output_csv_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-1233030698285661>, line 4\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Volumes/scratch/walt_wu/ganesh_project/test_round/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
       "\u001b[1;32m      3\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimageclassifier_2_layers_300*300.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
       "\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_save_path)\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "folder_path = '/Volumes/scratch/walt_wu/ganesh_project/test_round/'\n",
    "model_save_path = os.path.join('models', 'imageclassifier_2_layers_300*300.h5')\n",
    "model.save(model_save_path)\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Ensure inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "batchSizes = [16]  # Reduced batch sizes\n",
    "kernelSizes = [3]\n",
    "features = [[32, 32]]\n",
    "\n",
    "train_table = pd.DataFrame()\n",
    "test_CM = pd.DataFrame()\n",
    "test_result = pd.DataFrame()\n",
    "\n",
    "# Configure TensorFlow to allocate memory dynamically\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "for batchSize in batchSizes:\n",
    "    for kernelSize in kernelSizes:\n",
    "        for feature in features:\n",
    "            # for i in range(1, 11):\n",
    "            print(f'The following result is for batchSize {batchSize}, kernelSize {kernelSize}, and feature {feature}. Having 300*300 as cropped and has 2 layers iteration')\n",
    "            data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                directory='/Volumes/scratch/walt_wu/ganesh_project/Combine_dataset/',\n",
    "                labels='inferred',\n",
    "                label_mode='int',\n",
    "                class_names=None,\n",
    "                color_mode='rgb',\n",
    "                batch_size=batchSize,\n",
    "                image_size=(300, 300),\n",
    "                shuffle=True,\n",
    "                seed=None,\n",
    "                validation_split=None,\n",
    "                subset=None,\n",
    "                interpolation='bilinear',\n",
    "                follow_links=False,\n",
    "                crop_to_aspect_ratio=False,\n",
    "                pad_to_aspect_ratio=False,\n",
    "                data_format=None,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            # Extract class names (optional)\n",
    "            class_names = data.class_names\n",
    "\n",
    "            # Determine the number of classes\n",
    "            num_classes = len(class_names)\n",
    "\n",
    "            # One-hot encode the labels\n",
    "            def one_hot_encode_labels(images, labels):\n",
    "                return images, tf.one_hot(labels, depth=num_classes)\n",
    "\n",
    "            # Apply one-hot encoding to the dataset\n",
    "            one_hot_dataset = data.map(one_hot_encode_labels)\n",
    "            scaled_data = one_hot_dataset.map(lambda x, y: (x/255, y))\n",
    "            train_size = int((len(scaled_data)-1)*.7)\n",
    "            val_size = int((len(scaled_data))*.2)\n",
    "            test_size = len(scaled_data) - int((len(scaled_data)-1)*.1)\n",
    "            print(train_size, val_size, test_size)\n",
    "            train_data = scaled_data.take(train_size)\n",
    "            val_data = scaled_data.skip(train_size).take(val_size)\n",
    "            test_data = scaled_data.skip(train_size).skip(val_size)\n",
    "            print(test_data)\n",
    "\n",
    "            # Initialize the model\n",
    "            model = Sequential()\n",
    "\n",
    "            # Add Convolutional and Max Pooling layers\n",
    "            model.add(Conv2D(feature[0], (kernelSize, kernelSize), strides=1, activation='relu', input_shape=(300, 300, 3)))\n",
    "            model.add(MaxPooling2D())\n",
    "            model.add(Conv2D(feature[1], (kernelSize, kernelSize), strides=1, activation='relu'))\n",
    "            model.add(MaxPooling2D())\n",
    "\n",
    "            # Flatten the tensor output from the convolutional layers\n",
    "            model.add(Flatten())\n",
    "\n",
    "            # Add a Dense layer with 256 units and ReLU activation\n",
    "            model.add(Dense(256, activation='relu'))\n",
    "\n",
    "            # Add the output layer with `num_classes` units and softmax activation\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            logdir = 'logs'\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "            # Fit the model\n",
    "            hist = model.fit(train_data, validation_data=val_data, epochs=20, callbacks=[tensorboard_callback])\n",
    "\n",
    "            # Define the folder path where you want to save the plots\n",
    "            folder_path = '/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/uncropped/'\n",
    "            train_table[f'{batchSize}_{kernelSize}_{feature}_loss'] = hist.history['loss']\n",
    "            train_table[f'{batchSize}_{kernelSize}_{feature}_val_loss'] = hist.history['val_loss']\n",
    "            train_table[f'{batchSize}_{kernelSize}_{feature}_accuracy'] = hist.history['accuracy']\n",
    "            train_table[f'{batchSize}_{kernelSize}_{feature}_val_accuracy'] = hist.history['val_accuracy']\n",
    "\n",
    "            file_path = f'/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/uncropped/{batchSize}_{kernelSize}_{feature}_train_table.csv'\n",
    "\n",
    "            # Check if the file exists\n",
    "            if os.path.exists(file_path):\n",
    "                # Remove the file if it exists\n",
    "                os.remove(file_path)\n",
    "\n",
    "            # Save the model\n",
    "            model_save_path = os.path.join(f'/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/uncropped/', f'imageclassifier_2_layers_300*300_{batchSize}_{kernelSize}_{feature}.h5')\n",
    "            model.save(model_save_path)\n",
    "            print(f'Model saved to {model_save_path}')\n",
    "\n",
    "            # Save the dataframe to a CSV file with header\n",
    "            train_table.to_csv(file_path, index=False)\n",
    "            print(f'Train table saved to {file_path}')\n",
    "\n",
    "            # Plot Loss and save the figure\n",
    "            fig = plt.figure()\n",
    "            plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "            plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "            fig.suptitle('Loss', fontsize=20)\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            loss_plot_path = f'{folder_path}{batchSize}_{kernelSize}_{feature}_loss_plot.png'\n",
    "            plt.savefig(loss_plot_path)\n",
    "            plt.close(fig)\n",
    "            print(f'Loss plot saved to {loss_plot_path}')\n",
    "\n",
    "            # Plot Accuracy and save the figure\n",
    "            fig = plt.figure()\n",
    "            plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "            plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "            fig.suptitle('Accuracy', fontsize=20)\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            accuracy_plot_path = f'{folder_path}{batchSize}_{kernelSize}_{feature}_accuracy_plot.png'\n",
    "            plt.savefig(accuracy_plot_path)\n",
    "            plt.close(fig)\n",
    "            print(f'Accuracy plot saved to {accuracy_plot_path}')\n",
    "\n",
    "            test_result_l = []\n",
    "            correct = []\n",
    "            total_sample = []\n",
    "\n",
    "            num_classes = 4\n",
    "\n",
    "            # Create directory to save misclassified images if it doesn't exist\n",
    "            misclassified_dir = \"/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/uncropped/\"\n",
    "            if not os.path.exists(misclassified_dir):\n",
    "                os.makedirs(misclassified_dir)\n",
    "\n",
    "            for j in range(1, 31):\n",
    "                print(f'Test {j} starting')\n",
    "\n",
    "                # Initialize variables to track total correct predictions and total samples\n",
    "                total_correct_predictions = 0\n",
    "                total_samples = 0\n",
    "\n",
    "                y_true_all = []\n",
    "                y_pred_all = []\n",
    "\n",
    "                # Iterate over batches in test_data\n",
    "                for batch in test_data.as_numpy_iterator():\n",
    "                    X, y = batch\n",
    "                    yhat = model.predict(X)\n",
    "                    # Interpret model outputs: Choose the class with the highest probability\n",
    "                    predicted_indices = np.argmax(yhat, axis=-1)\n",
    "\n",
    "                    y_true_labels = np.argmax(y, axis=-1)\n",
    "\n",
    "                    y_true_all.extend(y_true_labels)\n",
    "                    y_pred_all.extend(predicted_indices)\n",
    "\n",
    "                    # Compare predicted classes to true labels\n",
    "                    for g, (pred, true_label) in enumerate(zip(predicted_indices, y_true_labels)):\n",
    "                        total_samples += 1\n",
    "                        if pred == true_label:\n",
    "                            total_correct_predictions += 1\n",
    "                        else:\n",
    "                            # Plot the misclassified image\n",
    "                            fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                            ax.imshow(X[g])\n",
    "                            ax.title.set_text(f'True: {true_label}, Predicted: {pred}')\n",
    "\n",
    "                            # Save the plot with a meaningful filename\n",
    "                            filename = os.path.join(misclassified_dir, f\"test_{j}_sample_{g}_true_{true_label}_pred_{pred}.png\")\n",
    "                            plt.savefig(filename)\n",
    "                            plt.close(fig)  # Close the figure to free up memory\n",
    "\n",
    "                            print(f\"Saved misclassified image: {filename}\")\n",
    "\n",
    "                # Calculate overall accuracy\n",
    "                overall_accuracy = total_correct_predictions / total_samples\n",
    "                print(f\"Overall Model Accuracy for test {j}: {overall_accuracy:.4f}\")\n",
    "                test_result_l.append(overall_accuracy)\n",
    "                correct.append(total_correct_predictions)\n",
    "                total_sample.append(total_samples)\n",
    "                test_result[f'overall_accuracy_for_{batchSize}_{kernelSize}_{feature}_iteration_test{j}'] = [overall_accuracy]\n",
    "                test_result[f'total_correct_predictions_for_{batchSize}_{kernelSize}_{feature}_iteration_test{j}'] = [total_correct_predictions]\n",
    "                test_result[f'total_samples_for_{batchSize}_{kernelSize}_{feature}_iteration_test{j}'] = [total_samples]\n",
    "\n",
    "                # Compute confusion matrix\n",
    "                conf_mat = confusion_matrix(y_true_all, y_pred_all, labels=list(range(num_classes)))\n",
    "                conf_mat_df = pd.DataFrame(conf_mat, index=class_names, columns=class_names)\n",
    "                test_CM[f'conf_mat_{batchSize}_{kernelSize}_{feature}_iter_test_{j}'] = conf_mat_df.values.flatten()\n",
    "\n",
    "            save_path = f'/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/uncropped/{batchSize}_{kernelSize}_{feature}_iteration_test_results.csv'\n",
    "            if os.path.exists(save_path):\n",
    "                os.remove(save_path)\n",
    "            test_result.to_csv(save_path, index=False)\n",
    "            print(f'Test results saved to {save_path}')\n",
    "\n",
    "            # Save confusion matrices\n",
    "            conf_mat_save_path = f'/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/uncropped/confusion_matrix_{batchSize}_{kernelSize}_{feature}_iteration.csv'\n",
    "            if os.path.exists(conf_mat_save_path):\n",
    "                os.remove(conf_mat_save_path)\n",
    "            test_CM.to_csv(conf_mat_save_path, index=False)\n",
    "            print(f'Confusion matrix saved to {conf_mat_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Ensure inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "batchSizes = [8,16]  # Reduced batch sizes\n",
    "kernelSizes = [3]\n",
    "features = [[32, 32]]\n",
    "\n",
    "train_table = pd.DataFrame()\n",
    "test_CM = pd.DataFrame()\n",
    "test_result = pd.DataFrame()\n",
    "\n",
    "# Configure TensorFlow to allocate memory dynamically\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "for batchSize in batchSizes:\n",
    "    for kernelSize in kernelSizes:\n",
    "        for feature in features:\n",
    "            for i in range(1, 11):\n",
    "                print(f'The following result is for batchSize {batchSize}, kernelSize {kernelSize}, and feature {feature}. Having 300*300 as cropped and has 2 layers {i} iteration')\n",
    "                data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                    directory='/Volumes/scratch/walt_wu/ganesh_project/Cropped_Combine_dataset_300/',\n",
    "                    labels='inferred',\n",
    "                    label_mode='int',\n",
    "                    class_names=None,\n",
    "                    color_mode='rgb',\n",
    "                    batch_size=batchSize,\n",
    "                    image_size=(300, 300),\n",
    "                    shuffle=True,\n",
    "                    seed=None,\n",
    "                    validation_split=None,\n",
    "                    subset=None,\n",
    "                    interpolation='bilinear',\n",
    "                    follow_links=False,\n",
    "                    crop_to_aspect_ratio=False,\n",
    "                    pad_to_aspect_ratio=False,\n",
    "                    data_format=None,\n",
    "                    verbose=True\n",
    "                )\n",
    "\n",
    "                # Extract class names (optional)\n",
    "                class_names = data.class_names\n",
    "\n",
    "                # Determine the number of classes\n",
    "                num_classes = len(class_names)\n",
    "\n",
    "                # One-hot encode the labels\n",
    "                def one_hot_encode_labels(images, labels):\n",
    "                    return images, tf.one_hot(labels, depth=num_classes)\n",
    "\n",
    "                # Apply one-hot encoding to the dataset\n",
    "                one_hot_dataset = data.map(one_hot_encode_labels)\n",
    "                scaled_data = one_hot_dataset.map(lambda x, y: (x/255, y))\n",
    "                train_size = int((len(scaled_data)-1)*.7)\n",
    "                val_size = int((len(scaled_data))*.2)\n",
    "                test_size = len(scaled_data) - int((len(scaled_data)-1)*.1)\n",
    "                print(train_size, val_size, test_size)\n",
    "                train_data = scaled_data.take(train_size)\n",
    "                val_data = scaled_data.skip(train_size).take(val_size)\n",
    "                test_data = scaled_data.skip(train_size).skip(val_size)\n",
    "                print(test_data)\n",
    "\n",
    "                # Initialize the model\n",
    "                model = Sequential()\n",
    "\n",
    "                # Add Convolutional and Max Pooling layers\n",
    "                model.add(Conv2D(feature[0], (kernelSize, kernelSize), strides=1, activation='relu', input_shape=(300, 300, 3)))\n",
    "                model.add(MaxPooling2D())\n",
    "                model.add(Conv2D(feature[1], (kernelSize, kernelSize), strides=1, activation='relu'))\n",
    "                model.add(MaxPooling2D())\n",
    "\n",
    "                # Flatten the tensor output from the convolutional layers\n",
    "                model.add(Flatten())\n",
    "\n",
    "                # Add a Dense layer with 256 units and ReLU activation\n",
    "                model.add(Dense(256, activation='relu'))\n",
    "\n",
    "                # Add the output layer with `num_classes` units and softmax activation\n",
    "                model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "                # Compile the model\n",
    "                model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "                logdir = 'logs'\n",
    "                tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "                # Fit the model\n",
    "                hist = model.fit(train_data, validation_data=val_data, epochs=20, callbacks=[tensorboard_callback])\n",
    "\n",
    "                # Define the folder path where you want to save the plots\n",
    "                folder_path = '/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/test_result_tables/'\n",
    "                train_table[f'{batchSize}_{kernelSize}_{feature}_loss'] = hist.history['loss']\n",
    "                train_table[f'{batchSize}_{kernelSize}_{feature}_val_loss'] = hist.history['val_loss']\n",
    "                train_table[f'{batchSize}_{kernelSize}_{feature}_accuracy'] = hist.history['accuracy']\n",
    "                train_table[f'{batchSize}_{kernelSize}_{feature}_val_accuracy'] = hist.history['val_accuracy']\n",
    "\n",
    "                file_path = f'/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/training_table/{batchSize}_{kernelSize}_{feature}_iteration_{i}_train_table.csv'\n",
    "\n",
    "                # Check if the file exists\n",
    "                if os.path.exists(file_path):\n",
    "                    # Remove the file if it exists\n",
    "                    os.remove(file_path)\n",
    "\n",
    "                # Save the model\n",
    "                model_save_path = os.path.join(f'/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/Models/', f'imageclassifier_2_layers_300*300_{batchSize}_{kernelSize}_{feature}_iteration_{i}.h5')\n",
    "                # model.save(model_save_path)\n",
    "                print(f'Model saved to {model_save_path}')\n",
    "\n",
    "                # Save the dataframe to a CSV file with header\n",
    "                train_table.to_csv(file_path, index=False)\n",
    "                print(f'Train table saved to {file_path}')\n",
    "\n",
    "                # Plot Loss and save the figure\n",
    "                fig = plt.figure()\n",
    "                plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "                plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "                fig.suptitle('Loss', fontsize=20)\n",
    "                plt.legend(loc=\"upper left\")\n",
    "                loss_plot_path = f'{folder_path}{batchSize}_{kernelSize}_{feature}_loss_plot.png'\n",
    "                plt.savefig(loss_plot_path)\n",
    "                plt.close(fig)\n",
    "                print(f'Loss plot saved to {loss_plot_path}')\n",
    "\n",
    "                # Plot Accuracy and save the figure\n",
    "                fig = plt.figure()\n",
    "                plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "                plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "                fig.suptitle('Accuracy', fontsize=20)\n",
    "                plt.legend(loc=\"upper left\")\n",
    "                accuracy_plot_path = f'{folder_path}{batchSize}_{kernelSize}_{feature}_accuracy_plot.png'\n",
    "                plt.savefig(accuracy_plot_path)\n",
    "                plt.close(fig)\n",
    "                print(f'Accuracy plot saved to {accuracy_plot_path}')\n",
    "\n",
    "                test_result_l = []\n",
    "                correct = []\n",
    "                total_sample = []\n",
    "\n",
    "                num_classes = 4\n",
    "\n",
    "                # Create directory to save misclassified images if it doesn't exist\n",
    "                misclassified_dir = \"/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/wrongly_classify_images/\"\n",
    "                if not os.path.exists(misclassified_dir):\n",
    "                    os.makedirs(misclassified_dir)\n",
    "\n",
    "                for j in range(1, 31):\n",
    "                    print(f'Test {j} starting')\n",
    "\n",
    "                    # Initialize variables to track total correct predictions and total samples\n",
    "                    total_correct_predictions = 0\n",
    "                    total_samples = 0\n",
    "\n",
    "                    y_true_all = []\n",
    "                    y_pred_all = []\n",
    "\n",
    "                    # Iterate over batches in test_data\n",
    "                    for batch in test_data.as_numpy_iterator():\n",
    "                        X, y = batch\n",
    "                        yhat = model.predict(X)\n",
    "                        # Interpret model outputs: Choose the class with the highest probability\n",
    "                        predicted_indices = np.argmax(yhat, axis=-1)\n",
    "\n",
    "                        y_true_labels = np.argmax(y, axis=-1)\n",
    "\n",
    "                        y_true_all.extend(y_true_labels)\n",
    "                        y_pred_all.extend(predicted_indices)\n",
    "\n",
    "                        # Compare predicted classes to true labels\n",
    "                        for g, (pred, true_label) in enumerate(zip(predicted_indices, y_true_labels)):\n",
    "                            total_samples += 1\n",
    "                            if pred == true_label:\n",
    "                                total_correct_predictions += 1\n",
    "                            else:\n",
    "                                # Plot the misclassified image\n",
    "                                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                                ax.imshow(X[g])\n",
    "                                ax.title.set_text(f'True: {true_label}, Predicted: {pred}')\n",
    "\n",
    "                                # Save the plot with a meaningful filename\n",
    "                                filename = os.path.join(misclassified_dir, f\"test_{j}_sample_{g}_true_{true_label}_pred_{pred}.png\")\n",
    "                                plt.savefig(filename)\n",
    "                                plt.close(fig)  # Close the figure to free up memory\n",
    "\n",
    "                                print(f\"Saved misclassified image: {filename}\")\n",
    "\n",
    "                    # Calculate overall accuracy\n",
    "                    overall_accuracy = total_correct_predictions / total_samples\n",
    "                    print(f\"Overall Model Accuracy for test {j}: {overall_accuracy:.4f}\")\n",
    "                    test_result_l.append(overall_accuracy)\n",
    "                    correct.append(total_correct_predictions)\n",
    "                    total_sample.append(total_samples)\n",
    "                    test_result[f'overall_accuracy_for_{batchSize}_{kernelSize}_{feature}_iteration_{i}_test{j}'] = overall_accuracy\n",
    "                    test_result[f'total_correct_predictions_for_{batchSize}_{kernelSize}_{feature}_iteration_{i}_test{j}'] = total_correct_predictions\n",
    "                    test_result[f'total_samples_for_{batchSize}_{kernelSize}_{feature}_iteration_{i}_test{j}'] = total_samples\n",
    "\n",
    "                    # Compute confusion matrix\n",
    "                    conf_mat = confusion_matrix(y_true_all, y_pred_all, labels=list(range(num_classes)))\n",
    "                    conf_mat_df = pd.DataFrame(conf_mat, index=class_names, columns=class_names)\n",
    "                    test_CM[f'conf_mat_{batchSize}_{kernelSize}_{feature}_iter_{i}_test_{j}'] = conf_mat_df.values.flatten()\n",
    "\n",
    "                save_path = f'/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/test_result_tables/{batchSize}_{kernelSize}_{feature}_iteration_{i}_test_results.csv'\n",
    "                if os.path.exists(save_path):\n",
    "                    os.remove(save_path)\n",
    "                test_result.to_csv(save_path, index=False)\n",
    "                print(f'Test results saved to {save_path}')\n",
    "\n",
    "                # Save confusion matrices\n",
    "                conf_mat_save_path = f'/Volumes/scratch/walt_wu/ganesh_project/Golden_Path/test_result_tables/confusion_matrix_{batchSize}_{kernelSize}_{feature}_iteration_{i}.csv'\n",
    "                if os.path.exists(conf_mat_save_path):\n",
    "                    os.remove(conf_mat_save_path)\n",
    "                test_CM.to_csv(conf_mat_save_path, index=False)\n",
    "                print(f'Confusion matrix saved to {conf_mat_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = np.array([[0,0,1], [1,1,0],[0,1,0]])\n",
    "y_pred = np.array([[0,0,1], [1,0,1],[1,0,0]])\n",
    "\n",
    "labels = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "conf_mat_dict={}\n",
    "print(y_true[:, 2])\n",
    "for label_col in range(len(labels)):\n",
    "    y_true_label = y_true[:, label_col]\n",
    "    y_pred_label = y_pred[:, label_col]\n",
    "    print(y_true_label)\n",
    "    # print(y_pred_label)\n",
    "    conf_mat_dict[labels[label_col]] = confusion_matrix(y_pred=y_pred_label, y_true=y_true_label)\n",
    "\n",
    "\n",
    "for label, matrix in conf_mat_dict.items():\n",
    "    print(\"Confusion matrix for label {}:\".format(label))\n",
    "    print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Defect_Classification",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "started",
   "language": "python",
   "name": "started"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
